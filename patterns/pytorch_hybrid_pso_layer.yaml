plse_version: "2.0"
pattern_id: "pytorch.layers.hybrid_pso_attention"

metadata:
  author: "PLSE v2.0 Core Library (from VSMPSOAttn.ipynb)"
  description: |
    Demonstrates an advanced architectural pattern: a hybrid PyTorch layer that
    combines standard gradient-based parameters with a gradient-free Particle
    Swarm Optimization (PSO) update rule. This layer treats its attention heads
    as particles in a swarm, optimizing them with both backpropagation and PSO.
  tags: [pytorch, best-practice, pso, attention, hybrid-model, research]
  pedagogy:
    concept: "Hybrid Gradient/Gradient-Free Custom Layers"
    difficulty: "advanced"

instruction: "Implement a custom PyTorch `nn.Module` for a PSO-driven multi-head attention layer. The layer's QKV projection weights should be `nn.Parameter`s to allow gradient updates, but it must also contain a `pso_update` method that modifies these weights using a vectorized, gradient-free PSO rule."

parameters:
  class_name:
    type: "choice"
    description: "The name of the custom attention layer class."
    default: "PSOAttentionLayer"
    constraints:
      options: ["PSOAttentionLayer", "SwarmAttention", "HybridAttention"]
  inertia_weight:
    type: "choice"
    description: "The inertia weight (w) for the PSO algorithm."
    default: 0.8
    constraints:
      options: [0.7, 0.8, 0.9]
  cognitive_coeff:
    type: "choice"
    description: "The cognitive coefficient (c1) for the PSO algorithm."
    default: 0.1
    constraints:
      options: [0.1, 0.5, 1.0]

requires:
  - "torch"
  - "torch.nn as nn"
  - "torch.nn.functional as F"

template: |
  class {{ class_name }}(nn.Module):
      """
      A multi-head self-attention layer where the QKV projection weights for each
      head are treated as particles in a swarm, updated by both PSO and backpropagation.
      """
      def __init__(self, d_model: int, num_heads: int, w: float = {{ inertia_weight }}, c1: float = {{ cognitive_coeff }}, c2: float = 0.1):
          super().__init__()
          assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
          self.d_model, self.num_heads, self.d_head = d_model, num_heads, d_model // num_heads
          
          # PSO hyperparameters
          self.w, self.c1, self.c2 = w, c1, c2
          
          # Particle dimension for one head (Q, K, and V matrices flattened)
          particle_dim = 3 * self.d_model * self.d_head

          # --- State Initialization ---
          # 1. Positions (The actual QKV weights)
          # We use nn.Parameter so they are tracked by autograd and optimizers.
          initial_positions = torch.empty(self.num_heads, particle_dim)
          nn.init.xavier_uniform_(initial_positions)
          self.positions = nn.Parameter(initial_positions)

          # 2. Other PSO states (velocity, bests)
          # We use register_buffer as these are part of the layer's state but
          # are NOT trainable parameters for the optimizer.
          self.register_buffer('velocities', torch.zeros(self.num_heads, particle_dim))
          self.register_buffer('pbest_positions', torch.zeros(self.num_heads, particle_dim))
          self.register_buffer('pbest_fitness', torch.full((self.num_heads,), float('inf')))
          self.register_buffer('gbest_position', torch.zeros(particle_dim))
          self.register_buffer('gbest_fitness', torch.tensor(float('inf')))
          
          self.out_proj = nn.Linear(d_model, d_model)

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          batch_size, seq_len, _ = x.shape
          
          # Reshape particle positions into Q, K, V weight matrices
          qkv_weights = self.positions.view(self.num_heads, 3, self.d_model, self.d_head)
          W_q, W_k, W_v = qkv_weights[:, 0], qkv_weights[:, 1], qkv_weights[:, 2]
          
          # Project input x to Q, K, V for all heads simultaneously
          Q = torch.einsum('bsd,hde->bhse', x, W_q)
          K = torch.einsum('bsd,hde->bhse', x, W_k)
          V = torch.einsum('bsd,hde->bhse', x, W_v)
          
          # Use the efficient scaled_dot_product_attention
          context = F.scaled_dot_product_attention(Q, K, V)
          
          # Reshape and project output
          context = context.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.d_model)
          return self.out_proj(context)

      @torch.no_grad()
      def pso_update(self, swarm_fitness_per_head: torch.Tensor):
          """
          Performs a single, vectorized, gradient-free PSO update.
          This method should be called manually during the training loop.
          """
          # Update personal bests
          improvement_mask = swarm_fitness_per_head < self.pbest_fitness
          self.pbest_positions[improvement_mask] = self.positions[improvement_mask].clone()
          self.pbest_fitness[improvement_mask] = swarm_fitness_per_head[improvement_mask]

          # Update global best
          min_fitness_in_swarm, best_particle_idx = torch.min(self.pbest_fitness, dim=0)
          if min_fitness_in_swarm < self.gbest_fitness:
              self.gbest_fitness = min_fitness_in_swarm
              self.gbest_position = self.pbest_positions[best_particle_idx].clone()

          # Vectorized velocity and position update for the whole swarm
          r1 = torch.rand_like(self.positions)
          r2 = torch.rand_like(self.positions)
          
          cognitive_term = self.c1 * r1 * (self.pbest_positions - self.positions)
          social_term = self.c2 * r2 * (self.gbest_position - self.positions)
          
          self.velocities.copy_(self.w * self.velocities + cognitive_term + social_term)
          self.positions.data.copy_(self.positions.data + self.velocities)

validation:
  linter_checks: true
  unit_test_snippets:
    - |
      # Test instantiation and state management
      D_MODEL = 64
      NUM_HEADS = 4
      
      layer = {{ class_name }}(d_model=D_MODEL, num_heads=NUM_HEADS)
      
      # 1. Verify parameter vs. buffer
      assert layer.positions.requires_grad is True, "Positions (weights) should be trainable parameters."
      assert layer.velocities.requires_grad is False, "Velocities should be non-trainable buffers."
      assert layer.pbest_fitness.requires_grad is False, "Fitness scores should be non-trainable buffers."

      # 2. Verify forward pass
      dummy_input = torch.randn(2, 10, D_MODEL) # (Batch, SeqLen, Dim)
      output = layer(dummy_input)
      assert output.shape == (2, 10, D_MODEL), "Forward pass output shape is incorrect."

      # 3. Verify PSO update runs and modifies positions
      initial_pos_clone = layer.positions.data.clone()
      dummy_fitness = torch.rand(NUM_HEADS)
      layer.pso_update(dummy_fitness)
      assert not torch.equal(initial_pos_clone, layer.positions.data), "pso_update did not modify the positions."
      
      print("Hybrid PSO-Attention layer validated successfully.")
