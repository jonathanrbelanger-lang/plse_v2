plse_version: "2.0"
pattern_id: "pytorch.layers.positional_encoding"

metadata:
  author: "PLSE v2.0 Core Library (from VSMPSOAttn.ipynb)"
  description: |
    Demonstrates the implementation of the classic sinusoidal positional encoding
    for Transformer models, as described in "Attention Is All You Need". This
    pattern teaches how to inject sequence order information into a model and
    highlights the use of `register_buffer` for state that is not a trainable parameter.
  tags: [pytorch, transformer, nlp, best-practice, layers]
  pedagogy:
    concept: "Sinusoidal Positional Encoding"
    difficulty: "intermediate"

instruction: "Implement a PyTorch `nn.Module` for sinusoidal positional encoding. The class, named `{{ class_name }}`, should be initializable with `d_model`, `dropout`, and `max_len`, and must store the encoding matrix as a non-parameter buffer."

parameters:
  class_name:
    type: "choice"
    description: "The name of the PositionalEncoding class."
    default: "PositionalEncoding"
    constraints:
      options: ["PositionalEncoding", "SinusoidalEncodingLayer"]
  d_model:
    type: "choice"
    description: "The dimensionality of the model's embeddings."
    default: 128
    constraints:
      options: [128, 256, 512]
  dropout_rate:
    type: "choice"
    description: "The dropout rate to apply to the final embeddings."
    default: 0.1
    constraints:
      options: [0.1, 0.2]
  max_len:
    type: "choice"
    description: "The maximum sequence length to pre-compute encodings for."
    default: 5000
    constraints:
      options: [1024, 5000]

requires:
  - "torch"
  - "torch.nn as nn"
  - "math"

template: |
  class {{ class_name }}(nn.Module):
      """
      Injects sinusoidal positional encoding into the input tensor.
      This implementation is based on the original Transformer paper.
      """
      def __init__(self, d_model: int, dropout: float = {{ dropout_rate }}, max_len: int = {{ max_len }}):
          super().__init__()
          self.dropout = nn.Dropout(p=dropout)

          # Create the position and division term vectors
          position = torch.arange(max_len).unsqueeze(1)
          div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
          
          # Create the positional encoding matrix
          pe = torch.zeros(max_len, 1, d_model)
          pe[:, 0, 0::2] = torch.sin(position * div_term)
          pe[:, 0, 1::2] = torch.cos(position * div_term)
          
          # CRITICAL: Register 'pe' as a buffer. It's part of the model's state,
          # but it's not a parameter that should be trained by the optimizer.
          # This ensures it gets moved to the correct device (e.g., GPU) with `model.to(device)`.
          self.register_buffer('pe', pe)

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          """
          Args:
              x: Tensor, shape [sequence_length, batch_size, d_model]
          """
          # Add the positional encoding to the input tensor
          x = x + self.pe[:x.size(0)]
          return self.dropout(x)

validation:
  linter_checks: true
  unit_test_snippets:
    - |
      # Test the PositionalEncoding module
      D_MODEL = {{ d_model }}
      BATCH_SIZE = 4
      SEQ_LEN = 50

      # 1. Instantiate the layer
      pos_encoder = {{ class_name }}(d_model=D_MODEL)

      # 2. Verify that 'pe' is a buffer, not a parameter
      is_param = any('pe' in name for name, _ in pos_encoder.named_parameters())
      is_buffer = any('pe' in name for name, _ in pos_encoder.named_buffers())
      assert not is_param, "'pe' should not be a trainable parameter."
      assert is_buffer, "'pe' should be registered as a buffer."

      # 3. Test the forward pass
      dummy_input = torch.zeros(SEQ_LEN, BATCH_SIZE, D_MODEL)
      output = pos_encoder(dummy_input)

      # 4. Check output shape and content
      assert output.shape == (SEQ_LEN, BATCH_SIZE, D_MODEL), "Output shape is incorrect."
      assert not torch.all(output == 0), "Positional encoding was not added to the input."
      
      print("PositionalEncoding layer validation passed successfully.")
